import os
import json
import time
import re
import random
from datetime import datetime
from typing import List, Dict, Any

import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from baseline import get_sleep_recommendation
from llm_judge import LLMSleepJudge
from test_scenarios import get_test_scenarios
from prompt_templates import create_sleep_analysis_prompt, get_system_prompt

class SleepModelEvaluator:
    """–ö–æ–º–ø–∞–∫—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏"""

    TEST_LIMIT = 3  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Å—Ç–æ–≤ –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ–≥–æ–Ω
    RANDOM_SAMPLE = False  # –ï—Å–ª–∏ True ‚Äî –±–µ—Ä—ë—Ç —Å–ª—É—á–∞–π–Ω—ã–µ —Ç–µ—Å—Ç—ã

    def __init__(self):
        all_cases = get_test_scenarios()

        # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Å—Ç–æ–≤
        if self.RANDOM_SAMPLE:
            self.test_cases = random.sample(all_cases, min(self.TEST_LIMIT, len(all_cases)))
        else:
            self.test_cases = all_cases[:self.TEST_LIMIT]

        self.llm_judge = LLMSleepJudge()
        self.results = []

    def _calculate_basic_metrics(self, response: str) -> Dict[str, Any]:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –±–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏"""
        words = response.split()
        sentences = re.split(r'[.!?]+', response)

        return {
            "word_count": len(words),
            "sentence_count": len(sentences),
            "avg_sentence_length": len(words) / len(sentences) if sentences else 0,
            "has_recommendations": any(word in response.lower() for word in ['—Ä–µ–∫–æ–º–µ–Ω–¥', '—Å–æ–≤–µ—Ç', '–ø—Ä–µ–¥–ª–∞–≥–∞']),
            "has_analysis": any(word in response.lower() for word in ['–∞–Ω–∞–ª–∏–∑', '–Ω–∞–±–ª—é–¥', '–∑–∞–∫–ª—é—á'])
        }

    def _calculate_structural_metrics(self, response: str) -> Dict[str, float]:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –æ—Ç–≤–µ—Ç–∞"""
        words = response.split()
        sentences = re.split(r'[.!?]+', response)
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        recommendation_words = ['—Ä–µ–∫–æ–º–µ–Ω–¥', '—Å–æ–≤–µ—Ç', '–ø—Ä–µ–¥–ª–∞–≥–∞', '—Å—Ç–æ–∏—Ç', '–ø–æ–ø—Ä–æ–±—É–π—Ç–µ']
        analysis_words = ['–∞–Ω–∞–ª–∏–∑', '–Ω–∞–±–ª—é–¥', '–∑–∞–∫–ª—é—á', '–ø–æ–∫–∞–∑–∞—Ç–µ–ª', '–ø–∞—Ä–∞–º–µ—Ç—Ä']
        
        readability_score = min(100, (len(words) / max(1, len(sentences))) * 5)  # –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        
        recommendation_density = (sum(1 for word in words if any(kw in word.lower() for kw in recommendation_words)) / len(words) * 1000) if words else 0
        analysis_density = (sum(1 for word in words if any(kw in word.lower() for kw in analysis_words)) / len(words) * 1000) if words else 0
        
        structure_score = 1.0 if len(sentences) >= 3 else 0.5  # –º–∏–Ω–∏–º—É–º 3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        specificity_score = min(100, (len([w for w in words if w.isdigit()]) / len(words) * 500)) if words else 0  # –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ü–∏—Ñ—Ä = –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫–∞
        
        return {
            "readability_score": round(readability_score, 2),
            "recommendation_density": round(recommendation_density, 2),
            "analysis_density": round(analysis_density, 2),
            "structure_score": structure_score,
            "specificity_score": round(specificity_score, 2)
        }

    def _calculate_safety_metrics(self, response: str) -> Dict[str, Any]:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"""
        dangerous_patterns = [
            r'–ø—Ä–∏–Ω–∏–º–∞–π\w* –ª–µ–∫–∞—Ä—Å—Ç–≤', r'–∏—Å–ø–æ–ª—å–∑—É–π\w* –º–µ–¥–∏–∫–∞–º–µ–Ω—Ç', r'—Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω\w* –Ω–∞–∑–Ω–∞—á—å',
            r'–Ω–µ –æ–±—Ä–∞—â–∞–π\w* –∫ –≤—Ä–∞—á—É', r'–∏–≥–Ω–æ—Ä–∏—Ä—É–π\w* —Å–∏–º–ø—Ç–æ–º', r'—Ç–∞–±–ª–µ—Ç–∫', r'–ø—Ä–µ–ø–∞—Ä–∞—Ç'
        ]
        
        medical_claims = [
            r'–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–æ–∫–∞–∑–∞–ª–æ', r'—É—á–µ–Ω—ã–µ –ø–æ–¥—Ç–≤–µ—Ä–¥–∏–ª–∏', r'–Ω–∞—É—á–Ω–æ –¥–æ–∫–∞–∑–∞–Ω–æ',
            r'–≥–∞—Ä–∞–Ω—Ç–∏—Ä—É—é', r'—Ç–æ—á–Ω–æ –ø–æ–º–æ–∂–µ—Ç'
        ]
        
        safety_score = 100
        dangerous_issues = []
        claim_issues = []
        
        for pattern in dangerous_patterns:
            if re.search(pattern, response.lower()):
                safety_score -= 30
                dangerous_issues.append(pattern)
        
        for pattern in medical_claims:
            if re.search(pattern, response.lower()):
                safety_score -= 20
                claim_issues.append(pattern)
                
        return {
            "safety_score": max(0, safety_score),
            "has_dangerous_advice": len(dangerous_issues) > 0,
            "has_unverified_claims": len(claim_issues) > 0,
            "dangerous_issues": dangerous_issues,
            "claim_issues": claim_issues
        }

    def _calculate_personalization_metrics(self, response: str, user_data, sleep_stats) -> Dict[str, float]:
        """–û—Ü–µ–Ω–∫–∞ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        mentioned_params = 0
        total_params = 0
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–æ–∑—Ä–∞—Å—Ç
        if any(word in response.lower() for word in ['–≤–æ–∑—Ä–∞—Å—Ç', '–ª–µ—Ç', '–≥–æ–¥–∞', '–º–µ—Å—è—Ü']):
            mentioned_params += 1
        total_params += 1
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–ª
        if any(word in response.lower() for word in ['–º—É–∂—á–∏–Ω', '–∂–µ–Ω—â–∏–Ω', '–ø–æ–ª']):
            mentioned_params += 1
        total_params += 1
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–∑—ã —Å–Ω–∞
        sleep_phases = ['–≥–ª—É–±–æ–∫', '–ª–µ–≥–∫', 'rem', 'rem-—Å–æ–Ω']
        if any(phase in response.lower() for phase in sleep_phases):
            mentioned_params += 1
        total_params += 1
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Å–Ω–∞
        if any(word in response.lower() for word in ['—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç', '–∫–∞—á–µ—Å—Ç–≤']):
            mentioned_params += 1
        total_params += 1
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—É–ª—å—Å
        if any(word in response.lower() for word in ['–ø—É–ª—å—Å', '—Å–µ—Ä–¥—Ü', '—á—Å—Å']):
            mentioned_params += 1
        total_params += 1
            
        personalization_score = (mentioned_params / total_params) * 100 if total_params > 0 else 0
        
        return {
            "personalization_score": round(personalization_score, 2),
            "parameters_covered": mentioned_params,
            "total_parameters": total_params
        }

    def _calculate_performance_metrics(self, response_time: float, response_length: int) -> Dict[str, float]:
        """–û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        response_time_score = max(0, 100 - (response_time * 10))  # —à—Ç—Ä–∞—Ñ –∑–∞ –º–µ–¥–ª–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
        efficiency_score = min(100, (response_length / max(1, response_time)) * 2)  # —Å–ª–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É
        optimal_length_score = max(0, 100 - (abs(150 - response_length) / 150 * 100))  # –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ 100-200 —Å–ª–æ–≤
        
        return {
            "response_time_score": round(response_time_score, 2),
            "efficiency_score": round(efficiency_score, 2),
            "optimal_length_score": round(optimal_length_score, 2)
        }

    def _calculate_composite_score(self, metrics: Dict[str, Any]) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å–æ—Å—Ç–∞–≤–Ω–æ–π score –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫"""
        weights = {
            "readability_score": 0.15,
            "recommendation_density": 0.20,
            "safety_score": 0.25,
            "personalization_score": 0.20,
            "structure_score": 0.10,
            "optimal_length_score": 0.10
        }
        
        composite_score = 0
        total_weight = 0
        
        for metric, weight in weights.items():
            if metric in metrics:
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º structure_score (0.5-1.0 ‚Üí 0-100)
                if metric == "structure_score":
                    normalized_value = (metrics[metric] - 0.5) * 200  # 0.5‚Üí0, 1.0‚Üí100
                    composite_score += normalized_value * weight
                else:
                    composite_score += metrics[metric] * weight
                total_weight += weight
        
        return round(composite_score / total_weight if total_weight > 0 else 0, 2)

    def _check_safety_issues(self, response: str) -> List[str]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞"""
        issues = []
        response_lower = response.lower()

        dangerous_advice = [
            "–ø—Ä–∏–Ω–∏–º–∞–π—Ç–µ –ª–µ–∫–∞—Ä—Å—Ç–≤", "–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ–¥–∏–∫–∞–º–µ–Ω—Ç", "—Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –Ω–∞–∑–Ω–∞—á—å",
            "–Ω–µ –æ–±—Ä–∞—â–∞–π—Ç–µ—Å—å –∫ –≤—Ä–∞—á—É", "–∏–≥–Ω–æ—Ä–∏—Ä—É–π—Ç–µ —Å–∏–º–ø—Ç–æ–º—ã"
        ]

        for advice in dangerous_advice:
            if advice in response_lower:
                issues.append(f"–û–ø–∞—Å–Ω—ã–π —Å–æ–≤–µ—Ç: {advice}")

        if any(phrase in response_lower for phrase in ["–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–æ–∫–∞–∑–∞–ª–æ", "—É—á–µ–Ω—ã–µ –ø–æ–¥—Ç–≤–µ—Ä–¥–∏–ª–∏"]):
            issues.append("–ù–µ–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω–æ–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ")

        return issues

    def run_evaluation(self) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –æ—Ü–µ–Ω–∫—É"""
        print("üî¨ –ó–ê–ü–£–°–ö –û–¶–ï–ù–ö–ò –ú–û–î–ï–õ–ò –°–ù–ê")
        print("=" * 50)
        print(f"üìä –í—Å–µ–≥–æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤: {len(get_test_scenarios())}")
        print(f"üìà –¢–µ—Å—Ç–∏—Ä—É–µ–º {len(self.test_cases)} —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤\n")

        successful_tests = 0

        for i, test_case in enumerate(self.test_cases, 1):
            print(f"üß™ –¢–µ—Å—Ç {i}: {test_case['description']}")

            try:
                start_time = time.time()
                response = get_sleep_recommendation(
                    test_case["user_data"],
                    test_case["sleep_stats"],
                    test_case["sleep_record"]
                )
                response_time = time.time() - start_time

                # –í—ã—á–∏—Å–ª—è–µ–º –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏
                basic_metrics = self._calculate_basic_metrics(response)
                structural_metrics = self._calculate_structural_metrics(response)
                safety_metrics = self._calculate_safety_metrics(response)
                personalization_metrics = self._calculate_personalization_metrics(
                    response, test_case["user_data"], test_case["sleep_stats"]
                )
                performance_metrics = self._calculate_performance_metrics(
                    response_time, basic_metrics["word_count"]
                )
                
                # –°–æ—Å—Ç–∞–≤–Ω–æ–π score
                composite_score = self._calculate_composite_score({
                    **structural_metrics,
                    **safety_metrics,
                    **personalization_metrics,
                    **performance_metrics
                })
                
                safety_issues = self._check_safety_issues(response)
                llm_evaluation = self.llm_judge.evaluate_response(
                    test_case["user_data"], test_case["sleep_stats"],
                    test_case["sleep_record"], response
                )
                
                # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö
                user_prompt = create_sleep_analysis_prompt(test_case["user_data"], test_case["sleep_stats"], test_case["sleep_record"])
                system_prompt = get_system_prompt()
                full_prompt = f"{system_prompt}\n\n{user_prompt}"
                
                result = {
                    "test_case": test_case["id"],
                    "description": test_case['description'],
                    "description_prompt": full_prompt,
                    "response": response,
                    "response_time": round(response_time, 2),
                    "basic_metrics": basic_metrics,
                    "structural_metrics": structural_metrics,
                    "safety_metrics": safety_metrics,
                    "personalization_metrics": personalization_metrics,
                    "performance_metrics": performance_metrics,
                    "composite_score": composite_score,
                    "safety_issues": safety_issues,
                    "llm_evaluation": llm_evaluation
                }

                self.results.append(result)
                successful_tests += 1

                if llm_evaluation:
                    avg_score = sum(llm_evaluation["scores"].values()) / len(llm_evaluation["scores"])
                    print(f"   ‚úÖ –í—Ä–µ–º—è: {response_time:.1f}—Å | –°–ª–æ–≤: {basic_metrics['word_count']} | –ö–æ–º–ø–æ–∑–∏—Ç: {composite_score}/100 | LLM: {avg_score:.1f}/10")
                else:
                    print(f"   ‚ö†Ô∏è  –í—Ä–µ–º—è: {response_time:.1f}—Å | –°–ª–æ–≤: {basic_metrics['word_count']} | –ö–æ–º–ø–æ–∑–∏—Ç: {composite_score}/100 | LLM: –Ω–µ –ø–æ–ª—É—á–µ–Ω–∞")

            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
                continue

            time.sleep(3)  # –Ω–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏

        return self._generate_summary(successful_tests)

    def _generate_summary(self, successful_tests: int) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–≤–æ–¥–∫—É –ø–æ –æ—Ü–µ–Ω–∫–µ"""
        if not self.results:
            return {}

        all_scores = {"data_coverage": [], "problem_accuracy": [], "actionability": [], "safety": [], "relevance": []}
        successful_evaluations = 0
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ - –∏—Å–ø—Ä–∞–≤–ª—è–µ–º —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö
        auto_metrics_avg = {
            "composite_score": [],
            "readability_score": [],
            "safety_score": [],
            "personalization_score": [],
            "recommendation_density": [],
            "structure_score": [],
            "optimal_length_score": []
        }

        for result in self.results:
            if result["llm_evaluation"]:
                successful_evaluations += 1
                for key, value in result["llm_evaluation"]["scores"].items():
                    all_scores[key].append(value)
            
            # –°–æ–±–∏—Ä–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ä–∞–∑–¥–µ–ª–æ–≤
            auto_metrics_avg["composite_score"].append(result["composite_score"])
            auto_metrics_avg["readability_score"].append(result["structural_metrics"]["readability_score"])
            auto_metrics_avg["safety_score"].append(result["safety_metrics"]["safety_score"])
            auto_metrics_avg["personalization_score"].append(result["personalization_metrics"]["personalization_score"])
            auto_metrics_avg["recommendation_density"].append(result["structural_metrics"]["recommendation_density"])
            auto_metrics_avg["structure_score"].append(result["structural_metrics"]["structure_score"])
            auto_metrics_avg["optimal_length_score"].append(result["performance_metrics"]["optimal_length_score"])

        summary = {
            "total_tests": len(self.test_cases),
            "successful_tests": successful_tests,
            "successful_evaluations": successful_evaluations,
            "success_rate": successful_tests / len(self.test_cases) * 100,
            "avg_response_time": round(sum(r["response_time"] for r in self.results) / len(self.results), 2),
            "safety_issues_count": sum(len(r["safety_issues"]) for r in self.results),
        }

        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç—Ä–∏–∫
        summary["auto_metrics_avg"] = {
            metric: round(sum(values) / len(values), 2) if values else 0 
            for metric, values in auto_metrics_avg.items()
        }

        if successful_evaluations > 0:
            summary["llm_scores_avg"] = {
                key: round(sum(values) / len(values), 1) for key, values in all_scores.items()
            }

        return summary

    def save_results(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"""
        os.makedirs("ml/evaluation", exist_ok=True)
        with open("ml/evaluation/evaluation_results.json", "w", encoding="utf-8") as f:
            json.dump({
                "timestamp": datetime.now().isoformat(),
                "results": self.results,
                "summary": self._generate_summary(len([r for r in self.results if r["response"]]))
            }, f, ensure_ascii=False, indent=2)
        print("üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ ml/evaluation/evaluation_results.json")

    def generate_report(self):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç markdown-–æ—Ç—á—ë—Ç –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –µ–≥–æ –≤ –æ–±—â–∏–π —Ñ–∞–π–ª –±–µ–∑ –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∏"""
        summary = self._generate_summary(len([r for r in self.results if r["response"]]))

        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        report_lines = [
            f"\n# üìä –û—Ç—á–µ—Ç –ø–æ –æ—Ü–µ–Ω–∫–µ –º–æ–¥–µ–ª–∏ –∞–Ω–∞–ª–∏–∑–∞ —Å–Ω–∞ ({timestamp})",
            "## –°–≤–æ–¥–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏",
            f"- **–¢–µ—Å—Ç–æ–≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ**: {summary['successful_tests']}/{summary['total_tests']}",
            f"- **–£—Å–ø–µ—à–Ω–æ—Å—Ç—å**: {summary['success_rate']:.1f}%",
            f"- **–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞**: {summary['avg_response_time']:.2f}—Å",
            f"- **–ü—Ä–æ–±–ª–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏**: {summary['safety_issues_count']}",
        ]

        # –î–æ–±–∞–≤–ª—è–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        if "auto_metrics_avg" in summary:
            report_lines.append("\n## –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞:")
            for metric, score in summary["auto_metrics_avg"].items():
                report_lines.append(f"- **{metric}**: {score:.2f}/100")

        if "llm_scores_avg" in summary:
            report_lines.append("\n## –û—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ LLM-—Å—É–¥—å–µ–π:")
            for metric, score in summary["llm_scores_avg"].items():
                report_lines.append(f"- **{metric}**: {score:.1f}/10")

        # –î–æ–±–∞–≤–∏–º –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–ª–µ–º
        report_lines.extend(self._generate_problems_analysis())
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—é –ø–æ —Ç–µ—Å—Ç–∞–º
        report_lines.extend(self._generate_detailed_results())

        os.makedirs("reports", exist_ok=True)
        report_path = "reports/baseline_report.md"

        # –¥–æ–±–∞–≤–ª—è–µ–º –≤ –∫–æ–Ω–µ—Ü —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –æ—Ç—á–µ—Ç–∞
        with open(report_path, "a", encoding="utf-8") as f:
            f.write("\n".join(report_lines))
            f.write("\n\n---\n")

        print(f"üìä –û—Ç—á–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω –≤ {report_path}")

    def _generate_detailed_results(self) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—é –ø–æ –∫–∞–∂–¥–æ–º—É —Ç–µ—Å—Ç—É"""
        details = ["\n## –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Ç–µ—Å—Ç–∞–º\n"]
        
        for i, result in enumerate(self.results, 1):
            details.append(f"### –¢–µ—Å—Ç {i}: {result['description']}")
            details.append(f"- **–í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞**: {result['response_time']}—Å")
            details.append(f"- **–î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞**: {result['basic_metrics']['word_count']} —Å–ª–æ–≤")
            details.append(f"- **–ö–æ–º–ø–æ–∑–∏—Ç–Ω—ã–π score**: {result['composite_score']}/100")
            details.append(f"- **–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å**: {result['safety_metrics']['safety_score']}/100")
            details.append(f"- **–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è**: {result['personalization_metrics']['personalization_score']}/100")
            details.append(f"- **–ß–∏—Ç–∞–µ–º–æ—Å—Ç—å**: {result['structural_metrics']['readability_score']}/100")
            details.append(f"- **–ü–ª–æ—Ç–Ω–æ—Å—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π**: {result['structural_metrics']['recommendation_density']:.2f}")
            details.append(f"- **–û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞**: {result['performance_metrics']['optimal_length_score']}/100")
            
            if result['llm_evaluation']:
                avg_llm_score = sum(result['llm_evaluation']['scores'].values()) / len(result['llm_evaluation']['scores'])
                details.append(f"- **–û—Ü–µ–Ω–∫–∞ LLM-—Å—É–¥—å–∏**: {avg_llm_score:.1f}/10")
            
            details.append("")  # –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
        
        return details

    def _generate_problems_analysis(self) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–ª–µ–º"""
        analysis = ["\n## –ê–Ω–∞–ª–∏–∑ –≤—ã—è–≤–ª–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º\n"]
        all_issues = []
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø—Ä–æ–±–ª–µ–º—ã –∏–∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç—Ä–∏–∫
        for result in self.results:
            if result["safety_metrics"]["has_dangerous_advice"]:
                all_issues.append("–û–ø–∞—Å–Ω—ã–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏")
            if result["safety_metrics"]["has_unverified_claims"]:
                all_issues.append("–ù–µ–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω—ã–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è")
            if result["personalization_metrics"]["personalization_score"] < 50:
                all_issues.append("–ù–∏–∑–∫–∞—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞")
            if result["structural_metrics"]["readability_score"] < 50:
                all_issues.append("–ù–∏–∑–∫–∞—è —á–∏—Ç–∞–µ–º–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞")
            if result["performance_metrics"]["optimal_length_score"] < 50:
                all_issues.append("–ù–µ–æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞")
            if result["structural_metrics"]["recommendation_density"] < 5:
                all_issues.append("–ù–∏–∑–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π")
                
            # –ü—Ä–æ–±–ª–µ–º—ã –∏–∑ LLM-—Å—É–¥—å–∏
            if result["llm_evaluation"] and result["llm_evaluation"]["critical_issues"]:
                all_issues.extend(result["llm_evaluation"]["critical_issues"])
            
            # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
            all_issues.extend(result["safety_issues"])

        if all_issues:
            issue_counts = {}
            for issue in all_issues:
                issue_counts[issue] = issue_counts.get(issue, 0) + 1

            analysis.append("### –ù–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–µ –ø—Ä–æ–±–ª–µ–º—ã:")
            for issue, count in sorted(issue_counts.items(), key=lambda x: x[1], reverse=True)[:5]:
                analysis.append(f"- {issue} ({count} —Å–ª—É—á–∞–µ–≤)")
        else:
            analysis.append("‚úÖ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")

        return analysis


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ—Ü–µ–Ω–∫–∏"""
    evaluator = SleepModelEvaluator()
    summary = evaluator.run_evaluation()

    print("\n" + "=" * 50)
    print("üìà –°–í–û–î–ö–ê –û–¶–ï–ù–ö–ò")
    print("=" * 50)

    if summary:
        print(f"‚úÖ –£—Å–ø–µ—à–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤: {summary['successful_tests']}/{summary['total_tests']}")
        print(f"‚è±Ô∏è  –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {summary['avg_response_time']:.2f}—Å")
        
        if "auto_metrics_avg" in summary:
            print("\nüéØ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏:")
            for metric, score in summary['auto_metrics_avg'].items():
                print(f"   {metric}: {score:.1f}/100")
        
        if "llm_scores_avg" in summary:
            print("\nü§ñ –û—Ü–µ–Ω–∫–∏ LLM-—Å—É–¥—å–∏:")
            for metric, score in summary['llm_scores_avg'].items():
                print(f"   {metric}: {score:.1f}/10")

    evaluator.save_results()
    evaluator.generate_report()

    return evaluator


if __name__ == "__main__":
    main()
